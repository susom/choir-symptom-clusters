# Hierarchical Modeling with Processed Data
# 
# Author: Eric Cramer <emcramer@stanford.edu>
# 
# Description:
# Applies a hierarchical clustering method to the data generated by 
# `0_preprocess_data.R` 
# 

# helper functions
# get the # of clusters with the optimal gap statistic
get_gap_stat <- function(x){
  k <- which(diff(x) < 0)[1] # get the first local maxima
  if(k == 1) {
    return(k)
  }
  i <- 1
  d <- x[k] - x[k - i] # check the difference with lesser values of k
  while(d < sderr(x)) {
    i <- i + 1
    d <- x[k] - x[k - i]
  }
  return(k - i + 1)
}

# helper function for plotting cluster metric distribution
plot_nbclust_hist <- function(nb) {
  bc <- data.frame(table(nb$Best.nc[1, ]))
  colnames(bc) <- c("N Clusters", "Count")
  p <- ggplot(bc, aes(`N Clusters`, Count)) +
    geom_bar(
      stat = 'identity'
      , fill = "grey"
      , color = 'black'
    ) + 
    labs(title = "Optimal Number of Clusters by 30 Metrics"
         , x = "Number of Clusters (k)"
    ) +
    theme_bw()
  return(p)
}

# plot each of the cluster metrics' descent in facet
plot_faceted_metrics <- function(nb) {
  df <- as.data.frame(nb$All.index)
  df$k <- 2:10
  df <- df %>%
    pivot_longer(
      -k
      , names_to = "metric"
      , values_to = "value")
  p <- ggplot(data = df) + 
    geom_point(aes(k, value), size = 0.5) + 
    facet_wrap(~metric, scales = "free_y") + 
    labs(title = "Clustering Criteria", x = "Number of Clusters", y = "Criterion Score") +
    theme_bw() + 
    theme(
      axis.text.y = element_blank()
      , axis.ticks.y = element_blank())
  return(p)
}

hierarchical_modeling <- function(){
  #--------------------------------------------------
  # LIBRARIES AND WORKSPACE
  #--------------------------------------------------
  
  # Load the required packages
  libs <- c("tidyverse", "glue", "magrittr", "here", 
            "cluster", "dendextend", "ggplot2",
            "factoextra", "NbClust", "FactoMineR",
            "fpc", "ggdendro")
  suppressPackageStartupMessages(easypackages::libraries(libs))
  
  # source utility functions
  utility_dir <- here("1_code/utilities")
  if(length(list.files(utility_dir)) > 0) {
    utility_files <- paste(utility_dir,list.files(utility_dir), sep="/")
    invisible(sapply(utility_files, source))
  }
  
  #--------------------------------------------------
  # LOADING DATA
  #--------------------------------------------------
  load(file = here("0_data/clean/data.RData"))
  
  # isolate the PROMIS measures as predictors
  pred_data <- train %>% 
    dplyr::select(all_of(ind_vars)) #%>%
  # sample_n(100)
  
  #--------------------------------------------------
  # MODELING
  #--------------------------------------------------
  # GRID SEARCH
  # generate the search grid for the hierarchical clustering
  gs <- list(hc_metric = c("euclidean"),
             hc_method = c("ward.D", "ward.D2", "single", "complete", 
                           "average", "mcquitty")) %>%
    cross_df()
  
  # set up the function to apply over the datset
  cluster_foo <- function(...){
    eclust(pred_data, FUNcluster = "hclust", ...)
  }
  
  gsx <- NULL
  
  # check if the computation has been done already
  if(!file.exists(here("2_output/models/grid_search.rds"))){
    # hierarchical clustering of the PROMIS measures over search grid
    gsx <- gs %>%
      mutate(fit = purrr::pmap(gs, cluster_foo)) %>%
      add_column(ac = unlist(lapply(.$fit, coef)))
    
    # save the fit dataframe
    saveRDS(gsx, here("2_output/models/grid_search.rds"))
  } else{
    gsx <- readRDS(here("2_output/models/grid_search.rds"))
  }
  
  # evaluate the models from the grid search
  gs_eval <- select(gsx, hc_metric, hc_method, ac)
  write_csv(gs_eval, here("2_output/tables/grid_search_results.csv"))
  
  # isolate the model with the highest ac
  hc_res <- gsx$fit[[which.max(gs_eval$ac)]]
  saveRDS(hc_res, here("2_output/models/hc_res.rds"))
  
  # get the optimum number of clusters by gap statistic
  opt_k <- get_gap_stat(hc_res$gap_stat$Tab[, 3])
  
  # then collect a series of other metrics
  nb_res <- NbClust(pred_data
                    , distance = 'euclidean'
                    , min.nc = 2
                    , max.nc = 10
                    , method = gs_eval$hc_method[which.max(gs_eval$ac)]
                    , index = 'all')
  saveRDS(nb_res, here("2_output/models/nbclust.rds"))
  
  #--------------------------------------------------
  # MODEL SELECTION & PLOTTING
  #--------------------------------------------------
  # plot the gap statistic
  gs_plot <- fviz_gap_stat(hc_res$gap_stat)
  gs_plot$layers[[1]]$aes_params$size <- 1.5
  gs_plot$layers[[2]]$aes_params$size <- 2
  gs_plot$layers[[3]]$aes_params$size <- 1
  gs_plot$layers[[4]]$aes_params$size <- 1
  gs_plot$layers[[1]]$aes_params$colour <- "black"
  gs_plot$layers[[2]]$aes_params$colour <- "black"
  gs_plot$layers[[3]]$aes_params$colour <- "black"
  gs_plot$layers[[4]]$aes_params$colour <- "red"
  gs_plot <- gs_plot +
    labs(x = "Number of clusters, k", y = "Gap statistic")
  saveRDS(gs_plot, here("2_output/img/gap_statistic.rds"))
  my_ggsave(gs_plot, here("2_output/img/gap_statistic"))
  
  # plot the NbClust results
  nb_hist <- plot_nbclust_hist(nb_res)
  my_ggsave(nb_hist, here("2_output/img/n_clusters_hist"))
  
  nb_facets <- plot_faceted_metrics(nb_res)
  my_ggsave(nb_facets, here("2_output/img/clustering_criteria"))
  
  # plot the dendrogram
  dend_plot <- hc_res %>% 
    as.dendrogram() %>%
    # set("branches_k_color", k = opt_k) %>%
    color_branches(h = 1000, col = c("#DEEBF7", "#3182BD", "#9ECAE1")) %>%
    set("branches_lwd", 2) %>%
    set("labels", NULL) %>%
    ggplot() +
    # scale_color_brewer(palette = "Blues") +
    ggtitle("Dendrogram of training data set") +
    annotate(
      "text"
      , x = c(1300, 8700, 4200)
      , y = -150
      , label = c("Cluster1\nn=2943", "Cluster2\nn=5802", "Cluster3\nn=2703")
      , size = 1.5
      ) 
  # + 
  #   theme(
  #     plot.margin = unit(c(30, 30, 72.27, 30), "points")
  #   )
  saveRDS(dend_plot, here("2_output/img/train_dend.rds"))
  my_ggsave(dend_plot, here("2_output/img/train_dend"))
}
# 
# as.dendrogram() %>%
#   set("branches_k_color", k = 3) %>%
#   set("labels", NULL) %>%
#   as.ggdend() %>%
#   ggplot() +
#   labs(title = "Clustering with Pain Severity and K = 3"
#        , subtitle = paste0("Optimum # of Clusters is ", opt_k)
#        , y = "Tree Height"
#        , x = "Observations (Patients)"
#   ) + 
#   theme(axis.title.y = element_text(angle = 90))
# 
# ddata <- dendro_data(hc_res, "rectangle")
# ddata[["labels"]] <- cbind(ddata[["labels"]], Cluster = as.factor(hc_res$cluster))
# 
# p <- ggplot(segment(ddata)) +
#   geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_segment(data = ddata$segments %>%
#                  filter(yend == 0) %>%
#                  left_join(ddata[["labels"]], by = "x")
#                , aes(x=x, y=y.x, xend=xend, yend=yend, color = Cluster)) +
#   labs(
#     title = "Dendrogam of Train Data Set"
#     , x = "Observations/Patients" 
#     , y = "Tree Height"
#   ) + 
#   theme_minimal() +
#   theme(
#     panel.grid.major = element_blank()
#     , panel.grid.minor = element_blank()
#   )
# p
